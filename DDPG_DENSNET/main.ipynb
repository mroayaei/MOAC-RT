{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "sys.path.append(\"..\")\n",
        "from model_cpp.model_env_cpp import CellEnvironment, transform_densities\n",
        "import cppCellModel\n",
        "from DDPG_DENSNET.OUActionNoise import OUActionNoise\n",
        "from DDPG_DENSNET.algorithm import get_actor, get_critic, policy, update_target, learn\n",
        "import tensorflow as tf\n",
        "from DDPG_DENSNET.Buffer import Buffer\n",
        "# from model.cell_environment import CellEnvironment\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "env = CellEnvironment('segmentation', False, 'dose', 'AC', True)\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "#actor_model = load_model('tmp/actor_DENSNET.h5')\n",
        "#critic_model = load_model('tmp/critic_model_DENSNET.h5')\n",
        "\n",
        "actor_model.summary()\n",
        "critic_model.summary()\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.00006\n",
        "actor_lr  = 0.00004\n",
        "\n",
        "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes =3\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.0001\n",
        "\n",
        "buffer = Buffer(50000, 64)\n",
        "\n",
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "observation_dimensions = (50, 50, 3)\n",
        "mean_reward, terminals, episodes, mean_dose, mean_time = {}, {}, {}, {}, {}\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "    env = CellEnvironment('segmentation', False, 'dose', 'AC', True)\n",
        "    _ = env.reset(-1)\n",
        "\n",
        "    obs_dim1 = np.array(env.observe()).squeeze() * (255.0)\n",
        "    obs_dim2 = cppCellModel.observeGlucose(env.controller_capsule) *(255/5300)\n",
        "    obs_dim3 = cppCellModel.observeOxygen(env.controller_capsule) *(255/170000)\n",
        "    prev_state = tf.convert_to_tensor(np.array([obs_dim1, obs_dim2, obs_dim3]).reshape((50, 50, 3)))\n",
        "\n",
        "    episodic_reward = 0\n",
        "    iter = 1\n",
        "    sum_dose = 0\n",
        "    sum_time = 0\n",
        "\n",
        "    # early stopping\n",
        "    best_reward = -np.inf\n",
        "    patience = 50\n",
        "    no_improvement_count = 0\n",
        "    ep_list = []\n",
        "    avg_ep = []\n",
        "\n",
        "    while True:\n",
        "\n",
        "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        cond=True\n",
        "\n",
        "        std_dev1 = 0.001\n",
        "        std_dev2 = 0.001\n",
        "\n",
        "        ou_noise = [OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev1) * np.ones(1)),\n",
        "                    OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev2) * np.ones(1))]\n",
        "\n",
        "        action ,saction = policy(actor_model, tf_prev_state, ou_noise, cond)\n",
        "        # Recieve state and reward from environment.\n",
        "        # print(\"Acting ...\")\n",
        "        reward, dose, time, KH = env.act(action)\n",
        "\n",
        "        obs_dim1 = np.array(env.observe()).squeeze() * (255.0)\n",
        "        obs_dim2 = cppCellModel.observeGlucose(env.controller_capsule) *(255/5300)\n",
        "        obs_dim3 = cppCellModel.observeOxygen(env.controller_capsule) *(255/170000)\n",
        "        state = tf.convert_to_tensor(np.array([obs_dim1, obs_dim2, obs_dim3]).reshape((50, 50, 3)))\n",
        "\n",
        "        done, which_terminal = env.inTerminalState()\n",
        "        # print(\"Recording ...\")\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "        print(\"Reward : {:.4f}  *  dose : {:.1f}  *  time : {:2}  *  epiReward : {:.4f}  *  Sampled Action : [{:.4f}, {:.4f}]  *  hcell_killed : {:5.2f}\".format(reward,dose,time,episodic_reward,saction[0],saction[1], KH))\n",
        "        if buffer.buffer_counter > 64:\n",
        "            # print(\"Learning ...\")\n",
        "            learn(buffer,\n",
        "                  target_actor, target_critic, critic_model, actor_model,\n",
        "                  critic_optimizer, actor_optimizer,\n",
        "                  gamma)\n",
        "            # print(\"Updating ...\")\n",
        "            update_target(target_actor.variables, actor_model.variables, tau)\n",
        "            update_target(target_critic.variables, critic_model.variables, tau)\n",
        "            # print(done)\n",
        "            # End this episode when `done` is True\n",
        "        if done:\n",
        "            terminals[ep] = which_terminal\n",
        "            episodes[ep] = iter\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "        sum_dose += dose\n",
        "        sum_time += time\n",
        "        iter += 1\n",
        "\n",
        "        ep_list.append(reward)\n",
        "        avg_ep = np.mean(ep_list[-30:])\n",
        "\n",
        "        if avg_ep > best_reward:\n",
        "            # actor_model.save('tmp/best_actor_DENSNET.h5')\n",
        "            # critic_model.save('tmp/best_critic_model_DENSNET.h5')\n",
        "            best_reward = avg_ep\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count >= patience:\n",
        "                print(f\"No improvement for {patience} episodes. Stopping early.\")\n",
        "                break\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    avg_reward = np.mean(ep_reward_list[-5:])\n",
        "    print(\"Episode * {:3} * Avg Reward is ==> {:.4f}  *  Last Reward is ==> {:.4f}\".format(ep, avg_reward, ep_reward_list[-1]))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "    mean_reward[ep + 1] = avg_reward\n",
        "    mean_dose[ep + 1] = sum_dose / iter\n",
        "    mean_time[ep + 1] = sum_time / iter\n",
        "\n",
        "    path = \"./tmp\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "    actor_model.save('tmp/actor_DENSNET.h5')\n",
        "    critic_model.save('tmp/critic_model_DENSNET.h5')\n",
        "\n",
        "    # save mean reward, terminals, num of iterations per episode, mean dose, mean time\n",
        "    with open('../DDPG_DENSNET/tmp/mean_reward_realtime.txt', 'w') as mean_reward_file:\n",
        "        mean_reward_file.write(json.dumps(mean_reward))\n",
        "\n",
        "    with open('../DDPG_DENSNET/tmp/episodes_realtime.txt', 'w') as episodes_file:\n",
        "        episodes_file.write(json.dumps(episodes))\n",
        "\n",
        "    with open('../DDPG_DENSNET/tmp/terminals_realtime.txt', 'w') as terminals_file:\n",
        "        terminals_file.write(json.dumps(terminals))\n",
        "\n",
        "    with open('../DDPG_DENSNET/tmp/mean_dose_realtime.txt', 'w') as mean_dose_file:\n",
        "        mean_dose_file.write(json.dumps(mean_dose))\n",
        "\n",
        "    with open('../DDPG_DENSNET/tmp/mean_time_realtime.txt', 'w') as mean_time_file:\n",
        "        mean_time_file.write(json.dumps(mean_time))\n",
        "\n",
        "path = \"./tmp\"\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "actor_model.save('tmp/actor_DENSNET.h5')\n",
        "critic_model.save('tmp/critic_model_DENSNET.h5')\n",
        "\n",
        "# save mean reward, terminals, num of iterations per episode, mean dose, mean time\n",
        "with open('../DDPG_DENSNET/tmp/mean_reward.txt', 'w') as mean_reward_file:\n",
        "    mean_reward_file.write(json.dumps(mean_reward))\n",
        "\n",
        "with open('../DDPG_DENSNET/tmp/episodes.txt', 'w') as episodes_file:\n",
        "    episodes_file.write(json.dumps(episodes))\n",
        "\n",
        "with open('../DDPG_DENSNET/tmp/terminals.txt', 'w') as terminals_file:\n",
        "    terminals_file.write(json.dumps(terminals))\n",
        "\n",
        "with open('../DDPG_DENSNET/tmp/mean_dose.txt', 'w') as mean_dose_file:\n",
        "    mean_dose_file.write(json.dumps(mean_dose))\n",
        "\n",
        "with open('../DDPG_DENSNET/tmp/mean_time.txt', 'w') as mean_time_file:\n",
        "    mean_time_file.write(json.dumps(mean_time))\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(avg_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vCCaCHBjgbcP"
      },
      "id": "vCCaCHBjgbcP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08609bcc-3a20-4068-87c1-f793c340de8d",
      "metadata": {
        "id": "08609bcc-3a20-4068-87c1-f793c340de8d"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72871efd-477d-48e3-a47e-5fea81f45f3a",
      "metadata": {
        "id": "72871efd-477d-48e3-a47e-5fea81f45f3a"
      },
      "outputs": [],
      "source": [
        "action ,saction = policy(actor_model, tf_prev_state, ou_noise, cond=False)\n",
        "action, saction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52b533dc-93ac-4c76-95e5-c0bb0e1cceff",
      "metadata": {
        "id": "52b533dc-93ac-4c76-95e5-c0bb0e1cceff"
      },
      "outputs": [],
      "source": [
        "actor_model.layers[-2].weights[0].numpy().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a57563ac-6a1e-4a04-9e0c-02f3209390e1",
      "metadata": {
        "id": "a57563ac-6a1e-4a04-9e0c-02f3209390e1"
      },
      "outputs": [],
      "source": [
        "np.clip(9.9970794e-01, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a34143d-1b39-49c2-a3f2-27c5ed7e0991",
      "metadata": {
        "id": "9a34143d-1b39-49c2-a3f2-27c5ed7e0991"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "env = CellEnvironment('segmentation', False, 'dose', 'AC', True)\n",
        "X=np.zeros((100,50,50,3))\n",
        "for i in tqdm(range(100)):\n",
        "    _ = env.reset(-1)\n",
        "    obs_dim1 = np.array(env.observe()).squeeze()\n",
        "    obs_dim2 = cppCellModel.observeGlucose(env.controller_capsule)\n",
        "    obs_dim3 = cppCellModel.observeOxygen(env.controller_capsule)\n",
        "    X[i,:,:,0]=obs_dim1\n",
        "    X[i,:,:,1]=obs_dim2\n",
        "    X[i,:,:,2]=obs_dim3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "275aed30-7949-4584-b5dc-90230f9680ca",
      "metadata": {
        "id": "275aed30-7949-4584-b5dc-90230f9680ca"
      },
      "outputs": [],
      "source": [
        "plt.plot(avg_reward_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507d4090-b339-4ffb-b1d0-e0f21f7aac30",
      "metadata": {
        "id": "507d4090-b339-4ffb-b1d0-e0f21f7aac30"
      },
      "outputs": [],
      "source": [
        "reward, dose, time = env.act(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dbcefd6-dec3-455c-be1a-53cf0a2c97ea",
      "metadata": {
        "id": "0dbcefd6-dec3-455c-be1a-53cf0a2c97ea"
      },
      "outputs": [],
      "source": [
        "reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac380157-6ab3-41ab-b720-3e7a65ad83f8",
      "metadata": {
        "id": "ac380157-6ab3-41ab-b720-3e7a65ad83f8"
      },
      "outputs": [],
      "source": [
        "X[:,:,:,0].max(), X[:,:,:,1].max(), X[:,:,:,2].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ddf83a-ce55-4786-9bea-8259f872d7d3",
      "metadata": {
        "id": "e3ddf83a-ce55-4786-9bea-8259f872d7d3"
      },
      "outputs": [],
      "source": [
        "X[:,:,:,0].max(), X[:,:,:,1].max(), X[:,:,:,2].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb7d1e96-492e-45af-9618-5026426e43a1",
      "metadata": {
        "id": "cb7d1e96-492e-45af-9618-5026426e43a1"
      },
      "outputs": [],
      "source": [
        "X[:,:,:,0].max(), X[:,:,:,1].max(), X[:,:,:,2].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d19c5a-6277-43b0-bbcd-390fa964741e",
      "metadata": {
        "id": "e4d19c5a-6277-43b0-bbcd-390fa964741e"
      },
      "outputs": [],
      "source": [
        "plt.hist(X[:,:,:,0].flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fa46c6c-bc4c-4a54-98f5-75b13839cabe",
      "metadata": {
        "id": "1fa46c6c-bc4c-4a54-98f5-75b13839cabe"
      },
      "outputs": [],
      "source": [
        "action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e6c7edf-2ee6-48b7-bd10-af8762802e42",
      "metadata": {
        "id": "7e6c7edf-2ee6-48b7-bd10-af8762802e42"
      },
      "outputs": [],
      "source": [
        "actor_model.save('tmp/actor_DENSNET.h5')\n",
        "critic_model.save('tmp/critic_model_DENSNET.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398fc556-e6e8-426f-9388-9aa11b6cc24d",
      "metadata": {
        "id": "398fc556-e6e8-426f-9388-9aa11b6cc24d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append(\"..\")\n",
        "from DDPG_DENSNET.OUActionNoise import OUActionNoise\n",
        "std_dev = 0.2\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e36897-508f-430c-951e-22a49f626707",
      "metadata": {
        "id": "61e36897-508f-430c-951e-22a49f626707"
      },
      "outputs": [],
      "source": [
        "(np.abs(np.sin(a))*0.1*(1-(a/np.max(a))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a7cf850-8df7-4c15-83d9-a82bd6ba3820",
      "metadata": {
        "id": "9a7cf850-8df7-4c15-83d9-a82bd6ba3820"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "a=np.arange(50)\n",
        "plt.figure()\n",
        "plt.plot(np.abs(np.sin(a))*0.2*(1-(a/np.max(a))))\n",
        "plt.plot(np.abs(np.cos(a))*0.2*(1-(a/np.max(a))))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}